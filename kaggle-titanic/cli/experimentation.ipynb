{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is from a \"Getting Started\" competition from Kaggle [Titanic competition](https://www.kaggle.com/c/titanic)  to showcase how we can use Auto-ML along with datmo and docker, in order to track our work and make machine learning workflow reprocible and usable. Some part of data analysis is inspired from this [kernel](https://www.kaggle.com/sinakhorami/titanic-best-working-classifier)\n",
    "\n",
    "This approach can be categorized into following methods,\n",
    "\n",
    "1. Exploratory Data Analysis (EDA) \n",
    "2. Data Cleaning\n",
    "3. Using Auto-ML to figure out the best algorithm and hyperparameter\n",
    "\n",
    "During the process of EDA and feature engineering, we would be using datmo to create versions of work by creating snapshot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re as re\n",
    "\n",
    "train = pd.read_csv('./input/train.csv', header = 0, dtype={'Age': np.float64})\n",
    "test  = pd.read_csv('./input/test.csv' , header = 0, dtype={'Age': np.float64})\n",
    "full_data = [train, test]\n",
    "\n",
    "print (train.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Exploratory Data Analysis \n",
    "###### To understand how each feature has the contribution to Survive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### a. `Sex`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Sex  Survived\n",
      "0  female  0.742038\n",
      "1    male  0.188908\n"
     ]
    }
   ],
   "source": [
    "print (train[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### b. `Pclass`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pclass  Survived\n",
      "0       1  0.629630\n",
      "1       2  0.472826\n",
      "2       3  0.242363\n"
     ]
    }
   ],
   "source": [
    "print (train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. `SibSp and Parch`\n",
    "\n",
    "With the number of siblings/spouse and the number of children/parents we can create new feature called Family Size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   FamilySize  Survived\n",
      "0           1  0.303538\n",
      "1           2  0.552795\n",
      "2           3  0.578431\n",
      "3           4  0.724138\n",
      "4           5  0.200000\n",
      "5           6  0.136364\n",
      "6           7  0.333333\n",
      "7           8  0.000000\n",
      "8          11  0.000000\n"
     ]
    }
   ],
   "source": [
    "for dataset in full_data:\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "print (train[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`FamilySize` seems to have a significant effect on our prediction. `Survived` has increased until a `FamilySize` of 4 and has decreased after that. Let's categorize people to check they are alone or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   IsAlone  Survived\n",
      "0        0  0.505650\n",
      "1        1  0.303538\n"
     ]
    }
   ],
   "source": [
    "for dataset in full_data:\n",
    "    dataset['IsAlone'] = 0\n",
    "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "print (train[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. `Embarked` \n",
    "\n",
    "we fill the missing values with most occured value `S`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Embarked  Survived\n",
      "0        C  0.553571\n",
      "1        Q  0.389610\n",
      "2        S  0.339009\n"
     ]
    }
   ],
   "source": [
    "for dataset in full_data:\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n",
    "print (train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. `Fare`\n",
    "\n",
    "Fare also has some missing values which will be filled with the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CategoricalFare  Survived\n",
      "0   (-0.001, 7.91]  0.197309\n",
      "1   (7.91, 14.454]  0.303571\n",
      "2   (14.454, 31.0]  0.454955\n",
      "3  (31.0, 512.329]  0.581081\n"
     ]
    }
   ],
   "source": [
    "for dataset in full_data:\n",
    "    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\n",
    "train['CategoricalFare'] = pd.qcut(train['Fare'], 4)\n",
    "print (train[['CategoricalFare', 'Survived']].groupby(['CategoricalFare'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows the `Fare` has a significant affect on survival, showcasing that people haivng paid higher fares had higher chances of survival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. `Age`\n",
    "\n",
    "There are plenty of missing values in this feature. # generate random numbers between (mean - std) and (mean + std). then we categorize age into 5 range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CategoricalAge  Survived\n",
      "0  (-0.08, 16.0]  0.521368\n",
      "1   (16.0, 32.0]  0.353468\n",
      "2   (32.0, 48.0]  0.372470\n",
      "3   (48.0, 64.0]  0.434783\n",
      "4   (64.0, 80.0]  0.090909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "for dataset in full_data:\n",
    "    age_avg = dataset['Age'].mean()\n",
    "    age_std = dataset['Age'].std()\n",
    "    age_null_count = dataset['Age'].isnull().sum()\n",
    "    \n",
    "    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n",
    "    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n",
    "    dataset['Age'] = dataset['Age'].astype(int)\n",
    "    \n",
    "train['CategoricalAge'] = pd.cut(train['Age'], 5)\n",
    "\n",
    "print (train[['CategoricalAge', 'Survived']].groupby(['CategoricalAge'], as_index=False).mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g. `Name`\n",
    "\n",
    "Let's get the title of people "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Title vs Sex=====\n",
      "Sex       female  male\n",
      "Title                 \n",
      "Capt           0     1\n",
      "Col            0     2\n",
      "Countess       1     0\n",
      "Don            0     1\n",
      "Dr             1     6\n",
      "Jonkheer       0     1\n",
      "Lady           1     0\n",
      "Major          0     2\n",
      "Master         0    40\n",
      "Miss         182     0\n",
      "Mlle           2     0\n",
      "Mme            1     0\n",
      "Mr             0   517\n",
      "Mrs          125     0\n",
      "Ms             1     0\n",
      "Rev            0     6\n",
      "Sir            0     1\n",
      "\n",
      "=====Title vs Survived=====\n",
      "       Title  Survived\n",
      "0       Capt  0.000000\n",
      "1        Col  0.500000\n",
      "2   Countess  1.000000\n",
      "3        Don  0.000000\n",
      "4         Dr  0.428571\n",
      "5   Jonkheer  0.000000\n",
      "6       Lady  1.000000\n",
      "7      Major  0.500000\n",
      "8     Master  0.575000\n",
      "9       Miss  0.697802\n",
      "10      Mlle  1.000000\n",
      "11       Mme  1.000000\n",
      "12        Mr  0.156673\n",
      "13       Mrs  0.792000\n",
      "14        Ms  1.000000\n",
      "15       Rev  0.000000\n",
      "16       Sir  1.000000\n"
     ]
    }
   ],
   "source": [
    "def get_title(name):\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Name'].apply(get_title)\n",
    "\n",
    "print(\"=====Title vs Sex=====\")\n",
    "print(pd.crosstab(train['Title'], train['Sex']))\n",
    "print(\"\")\n",
    "print(\"=====Title vs Survived=====\")\n",
    "print (train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's categorize it and check the title impact on survival rate convert the rare titles to `Rare`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Title  Survived\n",
      "0  Master  0.575000\n",
      "1    Miss  0.702703\n",
      "2      Mr  0.156673\n",
      "3     Mrs  0.793651\n",
      "4    Rare  0.347826\n"
     ]
    }
   ],
   "source": [
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n",
    "    'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "\n",
    "print (train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "config = {\"features analyzed\": [\"Sex\", \"Pclass\", \"FamilySize\", \"IsAlone\", \"Embarked\", \"Fare\", \"Age\", \"Title\"]}\n",
    "\n",
    "with open('config.json', 'w') as outfile:\n",
    "    json.dump(config, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: SAVE YOUR JUPYTER NOTEBOOK HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a datmo snapshot to save my work, this helps me save my current work before proceeding onto data cleaning \n",
    "\n",
    "Let's run this on terminal,\n",
    "\n",
    "```bash\n",
    "home:~/datmo-tutorials/kaggle-titanic$ datmo snapshot create -m \"EDA\"\n",
    "Creating a new snapshot\n",
    "Created snapshot with id: b978fce31d\n",
    "\n",
    "home:~/datmo-tutorials/kaggle-titanic$ datmo snapshot ls\n",
    "+------------+---------------------+------------------------------------------+-------+---------+-------+\n",
    "|     id     |      created at     |                  config                  | stats | message | label |\n",
    "+------------+---------------------+------------------------------------------+-------+---------+-------+\n",
    "| b978fce31d | 2018-06-03 19:40:50 |     {u'features analyzed': [u'Sex',      |   {}  |   EDA   |  None |\n",
    "|            |                     |  u'Pclass', u'FamilySize', u'IsAlone',   |       |         |       |\n",
    "|            |                     | u'Embarked', u'Fare', u'Age', u'Title']} |       |         |       |\n",
    "+------------+---------------------+------------------------------------------+-------+---------+-------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Data Cleaning\n",
    "Now let's clean our data and map our features into numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy  = train.copy()\n",
    "test_copy = test.copy()\n",
    "full_data_copy = [train_copy, test_copy]\n",
    "\n",
    "for dataset in full_data_copy:\n",
    "    # Mapping Sex\n",
    "    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "    \n",
    "    # Mapping titles\n",
    "    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
    "    dataset['Title'] = dataset['Title'].map(title_mapping)\n",
    "    dataset['Title'] = dataset['Title'].fillna(0)\n",
    "    \n",
    "    # Mapping Embarked\n",
    "    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n",
    "    \n",
    "    # Mapping Fare\n",
    "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare']                               = 0\n",
    "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
    "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n",
    "    dataset.loc[ dataset['Fare'] > 31, 'Fare']                                  = 3\n",
    "    dataset['Fare'] = dataset['Fare'].astype(int)\n",
    "    \n",
    "    # Mapping Age\n",
    "    dataset.loc[ dataset['Age'] <= 16, 'Age']                          = 0\n",
    "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n",
    "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n",
    "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n",
    "    dataset.loc[ dataset['Age'] > 64, 'Age']                           = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Survived  Pclass  Sex  Age  Fare  Embarked  IsAlone  Title\n",
      "0         0       3    1    1     0         0        0      1\n",
      "1         1       1    0    2     3         1        0      3\n",
      "2         1       3    0    1     1         0        1      2\n",
      "3         1       1    0    2     3         0        0      3\n",
      "4         0       3    1    2     1         0        1      1\n",
      "5         0       3    1    1     1         2        1      1\n",
      "6         0       1    1    3     3         0        1      1\n",
      "7         0       3    1    0     2         0        0      4\n",
      "8         1       3    0    1     1         0        0      3\n",
      "9         1       2    0    0     2         1        0      3\n"
     ]
    }
   ],
   "source": [
    "# Feature Selection\n",
    "drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp',\\\n",
    "                 'Parch', 'FamilySize']\n",
    "\n",
    "train_copy = train_copy.drop(drop_elements, axis = 1)\n",
    "train_copy = train_copy.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\n",
    "\n",
    "test_copy  = test_copy.drop(drop_elements, axis = 1)\n",
    "\n",
    "print (train_copy.head(10))\n",
    "\n",
    "train_copy = train_copy.values\n",
    "test_copy  = test_copy.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"selected features\": [\"Sex\", \"Pclass\", \"Age\", \"Fare\", \"Embarked\", \"Fare\", \"IsAlone\", \"Title\"]}\n",
    "\n",
    "with open('config.json', 'w') as outfile:\n",
    "    json.dump(config, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Using Auto-ML to figure out the best algorithm and hyperparameter\n",
    "##### Now we have cleaned our data it's time to use auto-ml in order to get the best algorithm for this data\n",
    "![](./images/usage_auto-ml.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  33%|███▎      | 100/300 [00:56<01:30,  2.21pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.826378965656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  50%|█████     | 150/300 [01:51<01:53,  1.33pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2 - Current best internal CV score: 0.835289799956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  67%|██████▋   | 200/300 [02:38<01:19,  1.26pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3 - Current best internal CV score: 0.835289799956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  83%|████████▎ | 250/300 [03:35<00:56,  1.13s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 4 - Current best internal CV score: 0.835289799956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 5 - Current best internal CV score: 0.835289799956\n",
      "\n",
      "Best pipeline: LogisticRegression(ExtraTreesClassifier(Normalizer(GaussianNB(input_matrix), norm=l1), bootstrap=False, criterion=entropy, max_features=0.4, min_samples_leaf=3, min_samples_split=14, n_estimators=100), C=25.0, dual=True, penalty=l2)\n",
      "0.766816143498\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train_copy[0::, 1::]\n",
    "y = train_copy[0::, 0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    train_size=0.75, test_size=0.25)\n",
    "\n",
    "tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2)\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))\n",
    "tpot.export('tpot_titanic_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {\"accuracy\": (tpot.score(X_test, y_test))} \n",
    "\n",
    "with open('stats.json', 'w') as outfile:\n",
    "    json.dump(stats, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE:  SAVE YOUR JUPYTER NOTEBOOK HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's again create a datmo snapshot to save my work, this helps me save my current work before changing my feature selection\n",
    "\n",
    "Let's run this on terminal,\n",
    "\n",
    "```\n",
    "home:~/datmo-tutorials/kaggle-titanic$ datmo snapshot create -m \"auto-ml-1\"\n",
    "Creating a new snapshot\n",
    "Created snapshot with id: 6ac8f41754\n",
    "\n",
    "home:~/datmo-tutorials/kaggle-titanic$ datmo snapshot ls\n",
    "+------------+----------------+------------------------------------------+---------------+-----------+-------+\n",
    "|     id     |   created at   |                  config                  |       stats   |  message  | label |\n",
    "+------------+----------------+------------------------------------------+---------------+-----------+-------+\n",
    "| 6ac8f41754 |   2018-06-03   |     {u'selected features': [u'Sex',      | {u'accuracy': | auto-ml-1 |  None |\n",
    "|            |    19:54:22    | u'Pclass', u'Age', u'Fare', u'Embarked', |     0.76}     |           |       |\n",
    "|            |                |     u'Fare', u'IsAlone', u'Title']}      |               |           |       |\n",
    "| b978fce31d |   2018-06-03   |     {u'features analyzed': [u'Sex',      |     {}        |    EDA    |  None |\n",
    "|            |    19:40:50    |  u'Pclass', u'FamilySize', u'IsAlone',   |               |           |       |\n",
    "|            |                | u'Embarked', u'Fare', u'Age', u'Title']} |               |           |       |\n",
    "+------------+----------------+------------------------------------------+---------------+-----------+-------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another feature selection\n",
    "1. Let's leave `FamilySize` rather than just unsing `IsAlone` \n",
    "2. Let's use `Fare_Per_Person` insted of binning `Fare`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy  = train.copy()\n",
    "test_copy = test.copy()\n",
    "full_data_copy = [train_copy, test_copy]\n",
    "\n",
    "for dataset in full_data_copy:\n",
    "    # Mapping Sex\n",
    "    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "    \n",
    "    # Mapping titles\n",
    "    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
    "    dataset['Title'] = dataset['Title'].map(title_mapping)\n",
    "    dataset['Title'] = dataset['Title'].fillna(0)\n",
    "    \n",
    "    # Mapping Embarked\n",
    "    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n",
    "    \n",
    "    # Mapping Fare\n",
    "    dataset['FarePerPerson']=dataset['Fare']/(dataset['FamilySize']+1)\n",
    "    \n",
    "    # Mapping Age\n",
    "    dataset.loc[ dataset['Age'] <= 16, 'Age']                          = 0\n",
    "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n",
    "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n",
    "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n",
    "    dataset.loc[ dataset['Age'] > 64, 'Age']                           = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Survived  Pclass  Sex  Age  Embarked  FamilySize  Title  FarePerPerson\n",
      "0         0       3    1    1         0           2      1       2.416667\n",
      "1         1       1    0    2         1           2      3      23.761100\n",
      "2         1       3    0    1         0           1      2       3.962500\n",
      "3         1       1    0    2         0           2      3      17.700000\n",
      "4         0       3    1    2         0           1      1       4.025000\n",
      "5         0       3    1    1         2           1      1       4.229150\n",
      "6         0       1    1    3         0           1      1      25.931250\n",
      "7         0       3    1    0         0           5      4       3.512500\n",
      "8         1       3    0    1         0           3      3       2.783325\n",
      "9         1       2    0    0         1           2      3      10.023600\n"
     ]
    }
   ],
   "source": [
    "# Feature Selection\n",
    "drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp',\\\n",
    "                 'Parch', 'IsAlone', 'Fare']\n",
    "\n",
    "train_copy = train_copy.drop(drop_elements, axis = 1)\n",
    "train_copy = train_copy.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\n",
    "\n",
    "test_copy  = test_copy.drop(drop_elements, axis = 1)\n",
    "\n",
    "print (train_copy.head(10))\n",
    "\n",
    "train_copy = train_copy.values\n",
    "test_copy  = test_copy.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  33%|███▎      | 100/300 [00:48<00:58,  3.41pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.835423966217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  50%|█████     | 150/300 [01:13<01:26,  1.73pipeline/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2 - Current best internal CV score: 0.836949671027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  67%|██████▋   | 200/300 [01:53<02:41,  1.62s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3 - Current best internal CV score: 0.839912800243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress:  83%|████████▎ | 250/300 [02:46<01:02,  1.25s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 4 - Current best internal CV score: 0.839912800243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 5 - Current best internal CV score: 0.839912800243\n",
      "\n",
      "Best pipeline: RandomForestClassifier(input_matrix, bootstrap=True, criterion=entropy, max_features=0.7, min_samples_leaf=2, min_samples_split=3, n_estimators=100)\n",
      "0.838565022422\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train_copy[0::, 1::]\n",
    "y = train_copy[0::, 0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    train_size=0.75, test_size=0.25)\n",
    "\n",
    "tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2)\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))\n",
    "tpot.export('tpot_titanic_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"selected features\": [\"Sex\", \"Pclass\", \"Age\", \"Fare\", \"Embarked\", \"FarePerPerson\", \"FamilySize\", \"Title\"]}\n",
    "\n",
    "with open('config.json', 'w') as outfile:\n",
    "    json.dump(config, outfile)\n",
    "\n",
    "stats = {\"accuracy\": (tpot.score(X_test, y_test))} \n",
    "\n",
    "with open('stats.json', 'w') as outfile:\n",
    "    json.dump(stats, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE:  SAVE YOUR JUPYTER NOTEBOOK HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's again create a datmo snapshot to save my final work\n",
    "\n",
    "Let's run this on terminal,\n",
    "\n",
    "```\n",
    "home:~/datmo-tutorials/kaggle-titanic$ datmo snapshot create -m \"auto-ml-2\"\n",
    "Creating a new snapshot\n",
    "Created snapshot with id: 2bcedce966\n",
    "\n",
    "home:~/datmo-tutorials/kaggle-titanic$ datmo snapshot ls\n",
    "+------------+----------------+------------------------------------------+---------------+-----------+-------+\n",
    "|     id     |   created at   |                  config                  |       stats   |  message  | label |\n",
    "+------------+----------------+------------------------------------------+---------------+-----------+-------+\n",
    "| 2bcedce966 |   2018-06-03   |     {u'selected features': [u'Sex',      | {u'accuracy': | auto-ml-2 |  None |\n",
    "|            |    20:31:35    | u'Pclass', u'Age', u'Fare', u'Embarked', |     0.83}     |           |       |\n",
    "|            |                |      u'FarePerPerson', u'FamilySize',    |               |           |       |\n",
    "|            |   \t         |                u'Title']}                |               |           |       |\n",
    "| 6ac8f41754 |   2018-06-03   |     {u'selected features': [u'Sex',      | {u'accuracy': | auto-ml-1 |  None |\n",
    "|            |    19:54:22    | u'Pclass', u'Age', u'Fare', u'Embarked', |     0.76}     |           |       |\n",
    "|            |                |     u'Fare', u'IsAlone', u'Title']}      |               |           |       |\n",
    "| b978fce31d |   2018-06-03   |     {u'features analyzed': [u'Sex',      |     {}        |    EDA    |  None |\n",
    "|            |    19:40:50    |  u'Pclass', u'FamilySize', u'IsAlone',   |               |           |       |\n",
    "|            |                | u'Embarked', u'Fare', u'Age', u'Title']} |               |           |       |\n",
    "+------------+----------------+------------------------------------------+---------------+-----------+-------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's now move to a different snapshot in order to either get the `experimentation.ipynb`, `submission.csv` or `tpot_titanice_pipeline.py` or any other files in that version. Since this will change the code as well, we should run this outside of the Jupyter notebook. You can save your Jupyter notebook here and run the code below in a new terminal. You should see your Jupyter notebook change to the previous version. \n",
    "\n",
    "We perform `checkout` command in order to achieve it\n",
    "\n",
    "```bash\n",
    "home:~/datmo-tutorials/auto-ml$ # Run this command: datmo snapshot checkout --id <snapshot-id>\n",
    "home:~/datmo-tutorials/auto-ml$ datmo snapshot checkout --id 30803662\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
